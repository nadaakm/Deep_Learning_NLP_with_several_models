{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## Toy example\n",
        "samples = ['I am the destroyer of worlds',\n",
        "           'The bringer of chaos',\n",
        "           'the phantom knight',\n",
        "           'i am your father',\n",
        "           'tHe GhOst In thE shell']\n",
        "\n",
        "vector_indice = {}\n",
        "for sentence in samples:\n",
        "  sentence = sentence.split(' ')\n",
        "  for word in sentence:\n",
        "    word = word.lower()\n",
        "    if word not in vector_indice:\n",
        "      vector_indice[word] = len(vector_indice)+1\n",
        "\n",
        "print(vector_indice)\n",
        "\n",
        "\n",
        "def vectoriser(phrase):\n",
        "  phrase_code = []\n",
        "  for word in phrase.split():\n",
        "    word_code = vector_indice[word.lower()]\n",
        "    phrase_code.append(word_code)\n",
        "  return phrase_code\n",
        "\n",
        "for phrase in samples:\n",
        "  print(vectoriser(phrase))"
      ],
      "metadata": {
        "id": "YU_XwPjSvlcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e4716ac-e9f0-40d9-fc87-6a679b7db6ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'i': 1, 'am': 2, 'the': 3, 'destroyer': 4, 'of': 5, 'worlds': 6, 'bringer': 7, 'chaos': 8, 'phantom': 9, 'knight': 10, 'your': 11, 'father': 12, 'ghost': 13, 'in': 14, 'shell': 15}\n",
            "[1, 2, 3, 4, 5, 6]\n",
            "[3, 7, 5, 8]\n",
            "[3, 9, 10]\n",
            "[1, 2, 11, 12]\n",
            "[3, 13, 14, 3, 15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5n127UvZlNG",
        "outputId": "0f1f5437-7f5d-450c-d5b9-540b9c073162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  66.3M      0  0:00:01  0:00:01 --:--:-- 66.3M\n"
          ]
        }
      ],
      "source": [
        "#get the dataset\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat aclImdb/train/pos/10000_8.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d3DHLn5cQP2",
        "outputId": "8a50768c-7f06-4e55-b648-ae65c76e0b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it's like to be homeless? That is Goddard Bolt's lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet's on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can't step off the sidewalk. He's given the nickname Pepto by a vagrant after it's written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They're survivors. Bolt isn't. He's not used to reaching mutual agreements like he once did when being rich where it's fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn't necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it's like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don't know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "ipgdZi4KeW8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nous commençons par charger l'ensemble de notre dataset"
      ],
      "metadata": {
        "id": "FPbD0yYMdOKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
        "train_dataset = text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=28,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=42\n",
        ")\n",
        "val_dataset =  text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=28,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=42\n",
        ")\n",
        "test_dataset = text_dataset_from_directory(\n",
        "    'aclImdb/test',\n",
        "    batch_size=28\n",
        ")\n",
        "#Voir le nombre de batches dans le dataset\n",
        "tf.data.experimental.cardinality(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLrt4jhndBvy",
        "outputId": "9e179441-8731-4773-9546-95dbc4a6b2ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int64, numpy=715>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On affiche le corpus qui est constitué de critiques de film"
      ],
      "metadata": {
        "id": "WjOwF2HWkM_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = {\n",
        "    0:\"negatif\",\n",
        "    1:\"positif\"\n",
        "}\n",
        "for text_batch,label_batch in train_dataset.take(1):\n",
        "  for i in range(5):\n",
        "    print(text_batch.numpy()[i])\n",
        "    classe = classes[label_batch.numpy()[i]]\n",
        "    print(\"the review is : {}\".format(classe))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbdS_jDDhKmc",
        "outputId": "10ee202b-b7cd-4f0b-fcc6-33f55430f513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"Rob Estes, Josie Bisset and a crap load of kids that look nothing like either of them.<br /><br />Basically, Rob and Josie have a shotgun wedding on a drunken night during a Vegas vacation. They each come home to find that their respective children already know of the nuptials due to tabloid-like not-so-fodder. They, Rob and Josie, move both of them and their eight kids into one or the other's house.<br /><br />Rob builds furniture, I think, which is close enough to Frank Lambert's (Patrick Duffy) construction job on the much similar Step by Step to warrant eternal mockage.<br /><br />Josie is some sort of cookie-making queen, though it doesn't look like she makes any of the cookies. Not close enough to Carol Foster's (Suzanne Somers)hairdressing job to warrant likeness mockage, but hilariously preposterous enough to warrant atrocity mockage.<br /><br />Unlike Step by Step, they were a couple before the vacation and actually knew one another's last names, or so one assumes if their serious enough about a relationship to take a trip together.<br /><br />Anyhow, there are eight kids; Moira, Sandy, Jeff, Lily, Daisy, Nathan, Andrew L. and Andrew B. I, personally, think they should've just called the younger Andrew 'Andy'.<br /><br />There's a lot of product placement, particularly for Soup at Hand (Which is disgusting) and Listerine Pocket Packs. There are also some stupid, senseless moments. It's also not a great film to promote happy families.<br /><br />But, hey! Rob Estes! This concludes my review of 'Step By Step... on some really bad drugs.' Watch it for Rob Estes and his pretty!eyes. There are some great pretty!eyes shots.\"\n",
            "the review is : positif\n",
            "b\"I was really looking forward to seeing this film, but after watching it I was really disappointed. The best bit was when Stephen King was in it. Rober John Berk cannot act to save his life and neither can any of the others. A few of the performances even made me laugh out loud! The film was was not as I imagined it, after reading the book which was awesome, I imagined it darker and a lot scarier. If i was Stephen, I would be really mad!<br /><br />I don't know why they changed the ending, I thought the ending of the book was very good. If you just found out the pie killed your daughter, you wouldn't feed it to anyone else would you?!<br /><br />Book was so much better!\"\n",
            "the review is : negatif\n",
            "b'What a lovely heart warming television movie. The story tells of a little five year old girl who has lost her daddy and finds it impossible to cope. Her mother is also very distressed ..only a miracle can alleviate their unhappiness.Which all viewers hope will materialise. Samantha Mathis is brilliant as the little girl\\'s mum ,as she was as the nanny in\" Jack and Sarah\",worth watching if you like both Samantha Mathis and happy; year tear jerking movies! Ellen Burstyn is, as, always a delightful grandmother in this tender and magnificently acted movie. Jodelle Ferland (the little five year old) is charming and a most convincing young actress. The film is based on a true story which makes it so touching.\"Mermaid\" is a tribute to the milk of human kindness which is clearly illustrated and clearly is still all around us in this difficult world we live in. \"Mermaid\" gives us all hope ,by realising that there a lot of lovely people in the world with lot\\'s of love to give. James Robson Glasgow Scotland U.K.'\n",
            "the review is : positif\n",
            "b'This movie should be shown to film school students as an example of what NOT to do. The original kicked some major tire squealing butt, this horrible disaster breaks the cardinal rule of Bruckheimer films, which is: we all know they suck, but they have great action. This film has NO ACTION. This film is BORING. Where are the cars? Where are the chases? Where\\'s the tension? Where\\'s the suspense? Where\\'s the rush? Where!!?? This isn\\'t really a movie at all, it\\'s a bad commercial. 50 cars in 24 hours? That is wrong. They have 3 days to steal them, the ad is wrong. How bad is that? The leads acting is stiff, wooden and forced. The villain, the cop, the others...who cares. They utter their pointless lines, they serve the illogical plot. They slog through it the best they can as the music video director says \"don\\'t worry we\\'ll make a lot of fast cuts and no one will notice how bad the film is\" or \"we\\'ll fix it with lots of loud music\" The \"script\" isn\\'t really a script at all, it\\'s more like a list of cliches with an ending that is a total ripoff of: -------Warning - possible spoiler------ 5------4-----3-----2-----1------ The Fugitive. The biggest crime of all is the underuse of Vinnie Jones, man....this is the baddest, coolest mofo since Jules in Pulp Fiction. And what do they do!? They make him a mute who\\'s hardly in the film! Make Vinnie the main villain and he could have saved the film. How could they have been so dumb? How? How? Why? The original film is very entertaining with a cool trick at the end that gets the driver away. The original has a great 40 minute chase that delivers! Go find the original. Or if you\\'re craving some real car chase action go rent RONIN. The chases in Ronin raised the bar by which all other car chases will now be judged. Bruckheimer and Cage had all that money, all those resources, all that experience, and they can\\'t even come close to matching a film made 25 years ago for $250,000? How can that be? You feel like you got ripped off after seeing this movie. Where I was once excited to see Coyote Ugly, Remember the Titans and Pearl Harbor, now I say: God help us all. <br /><br />'\n",
            "the review is : negatif\n",
            "b\"Elvira(Cassandra Peterson) is the host of a cheap horror show. After she finds out that her dead aunt has left her some stuff, elvira goes to England to pick it up, hoping it will be some money. But to her horror, elvira finds out that all her aunt has left her is her house, her dog and a cookbook. Elvira decides to settle in the house anyways, but with her striking dark looks and her stunning features, she will not be able to live in peace. All the neighbours are now turning the whole town against her, and with Elvira's outrageous attitude and looks, everyone better watch out, because Elvira is on Fire! I really enjoyed this movie, it's really fun to watch get Elvira into all these adventures, she's just great. The whole movie puts you into a halloween mood, sure, it's silly and the jokes are cheap but it's a pleasure to watch it. I would give Elvira, Mistress Of The Dark 8/10\"\n",
            "the review is : positif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nous allons retirer les balises HTML indésirables de notre corpus.\n"
      ],
      "metadata": {
        "id": "Bky3dsBykouF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def standardiser_text(texte):\n",
        "  lower = tf.strings.lower(texte) #Rendre mon texte minuscule\n",
        "  lower_striped = tf.strings.strip(lower) #Retirer les espaces de trailing\n",
        "  lower_striped = tf.strings.regex_replace(lower_striped,pattern='<[^>]+>',rewrite=' ')\n",
        "  return tf.strings.regex_replace(\n",
        "      lower_striped, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
        "  )"
      ],
      "metadata": {
        "id": "Iup4hspZiZ1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "couche_vecteur = tf.keras.layers.TextVectorization(\n",
        "    standardize=standardiser_text,\n",
        "    max_tokens=10000,\n",
        "    output_mode= 'int',\n",
        "    output_sequence_length = 200,\n",
        ")"
      ],
      "metadata": {
        "id": "AhYxMTiFoMfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = train_dataset.map(lambda text,label:text)\n",
        "couche_vecteur.adapt(corpus)"
      ],
      "metadata": {
        "id": "uqIQOAmOs6uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##On va appliquer la vectorisation à chaque dataset\n",
        "def vectoriser_texte(text,label):\n",
        "  text = tf.expand_dims(text,-1)\n",
        "  return couche_vecteur(text),label\n",
        "\n",
        "vector_train_dataset = train_dataset.map(vectoriser_texte)\n",
        "vector_val_dataset = val_dataset.map(vectoriser_texte)\n",
        "for text,label in vector_train_dataset.take(1):\n",
        "  for i in range(10):\n",
        "    print(type(text.numpy()[i]))\n",
        "    print(text.numpy()[i])\n",
        "    print('Ce texte est : {}'.format(classes[label.numpy()[i]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wkB1Mzst3aZ",
        "outputId": "d6603978-5784-4aa2-ef3c-3a1ff1e953f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "[ 419 4429    7  286    8   46 9200    1 2204 7543 1116   14  712  312\n",
            "  335 2697 4907    1 3676    1    1 4418 9144    1 1971  818    1 4739\n",
            " 1983    1   23 2876    4  313   20    2  509  667    8    2 7173  101\n",
            "    4    1    8    4 1483   16    4  864    1 1326 1631    1   34 1170\n",
            "    6  793   44    4 4412 1705   14   22   59  965   34   23 6926  906\n",
            "  163   44   12    2  265 3135    6    2    1 1326   36 1058    9    6\n",
            " 1674  512   79   15  966   60  691   90  809   68  690  436 2059    1\n",
            "    1  586   93   11 2371  359   13 8918 1118  522   32 2390 4539   36\n",
            "   77  201    4    1  300 5280 1078   14   73   28   43    6  131   12\n",
            "  419 4429    7  154  179  342  481  416  315   20    4 5549  595  191\n",
            "   19   12  180    6 1790   61    6 2348    8   20    2    1  981    5\n",
            " 7518 3474    2 4199    1   15 1674 1590   14   29  394  630 1668    2\n",
            " 9539 5638 2571   15  419 4429  787  172 4539 1075    9  101 2125   42\n",
            "  780 5226  667  265    8  419  388 7448   15   54  279   10    1   27\n",
            " 1075    9  101  146]\n",
            "Ce texte est : negatif\n",
            "<class 'numpy.ndarray'>\n",
            "[  51  146    4 2424  116   15    2   83   58    8    4  209  132   10\n",
            "  403  319   18   67    2 5471   16  229  149  654 2148   14  231   14\n",
            "    2  275  737  261   34   23   52  697   33    1  384   48   27   68\n",
            "    6 1151    8  132   77 3089    6   26  306    9   40  261    6  119\n",
            "   22   87   72  122   12   19   98   25   74  202   29 3767    1 1142\n",
            "    4  171    5   79 3310   16   69   51   10  131   12   10   62   38\n",
            "    1    1  765  242    9  144 1855    6 1719    2    1    5 2903   63\n",
            "  862    7    2  713   44    5   11  182  189    2   63    7 5097    3\n",
            "    2  100   23  386    6 1112   58   16   16  913    5  945    3    4\n",
            " 1441 2049    6 4747   10   98  103   11   28   40   14   72   14    2\n",
            " 2286   78 2170   47    7    4  171   42 1578 2104   35  721    6    1\n",
            "   30 4040  128  188  252  486  510 7352    6  372   24    1   56    6\n",
            "  586   24 8319    2 8319 7352    6  959   64  511 1651    4  156   95\n",
            "    5  573   15    2  701    3    2 5619 2424 1578   12   34   23   50\n",
            "   70   40 9672    1]\n",
            "Ce texte est : positif\n",
            "<class 'numpy.ndarray'>\n",
            "[ 404    4  777   60    1    2  523 3182 4205    3   78  785 5544  300\n",
            "    2   17 6136    4  585  111    1    5    4  800 2021   16   33  304\n",
            "  985    9  294    4  162  480 1396 2027   16 5698    3 2466  133   71\n",
            "   59    1  218  165  656   53  578   60   59   26  340    6  167 4051\n",
            "    9  266  167   87  968 7460   68 9063    2  530    5  468    3   87\n",
            "  952   34   68   26 5694    2  113  150   20   11   17    7   12   47\n",
            "    7   57  353  354  124   60   68  664   26 8816    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "Ce texte est : positif\n",
            "<class 'numpy.ndarray'>\n",
            "[  51   10   13    4  554   71  204  330    6   26    1    3   71  204\n",
            "  330    6  863    4   19   41   67    4   19   31    2  434   11    7\n",
            "   28    5    2   94   71  289   11    7   28    5    2 8994   94  195\n",
            "  121  105   10  102    9  227   53   26    4 6153  883 1213   19    4\n",
            " 7213    7 3357   20  680  487   81    4  390 5478    4  390    3    7\n",
            "  258  276   27   68    6   75  142  345   18   27    7 7241   32    2\n",
            " 3535    5    2  680    2   61   49  902   10   68  193    7    2 1074\n",
            "    5  151  142    6    2    1 1332 3213   14    2 7213 1599 1558  967\n",
            "    3    1 2008 4740   14 1670 7070 2722    1   14    1    1    3    1\n",
            " 7789   14    1   18  948   35   12   29  555  591  342    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "Ce texte est : negatif\n",
            "<class 'numpy.ndarray'>\n",
            "[   4 1302    8    2  447    7    4 4831  411    5  434 2329    3  930\n",
            "  147 4278    2  274   63    5 7082    1   36    8 4520 4913    4 2860\n",
            " 1757  311   39    1  868  108 4512  600   35   64 4890 9211  132 7419\n",
            "   20    2    1    1    1    2  868   13  110  105  169    3    2  929\n",
            "    5   39 3489 6982 2025    4  274  116 8676   30  183    2  182 4779\n",
            " 4318  120    1 1320    6    2  214    5 7082   14   55  204  120   18\n",
            "    2   28  150   12 1532    4 1302    8    2  447  110  759 1009    7\n",
            "    2  506  448    4  352 1534    3 1785    1   20   11    1  401 1662\n",
            "   23  842    6    2  511    2 1067    7  155  322   92 3160    3 3272\n",
            "   22   23  308 2292   18   77 1534    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "Ce texte est : positif\n",
            "<class 'numpy.ndarray'>\n",
            "[  10   62  455    6   38   11   17   18    9   13   40    1    2  109\n",
            "   13 7157 5529    2  111   13  592    3    2 1052   13  602    1    1\n",
            "    2  211   58 1218    8    2  741   10  455    2   17    6  124 2176\n",
            "    1   81    4   94    3  141    1   15   33  270  242   46    5    2\n",
            " 1988   65  316   18 7827  461   10  380   62    3   15    4   17   42\n",
            "    4  225    1 2551    1   47   62  269   72  222    6 1124    5    2\n",
            "  222   47   13   13  592    3  346   47   65  477 1305 9006  901   31\n",
            "  206   99  143   65   77  592 1346   11   19 1253    6 2739   69    3\n",
            " 5404   69   44   31    2  164   58  141 1194   29  126   10  349    6\n",
            "  137   67    1    3 8361   11   80 1244   44    5   54 1735    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "Ce texte est : negatif\n",
            "<class 'numpy.ndarray'>\n",
            "[  45   22  178    6  103  138   12    7   15   85    3   39   37    6\n",
            "  131   92   11    7    2   19    6 1271   10  237    4 6913   15    1\n",
            "    1   18   54  672    7   21  204   37 5915   48    4  225  564   10\n",
            " 1253    6   75   85    6  103    9   84   10  561   85    9   13   42\n",
            " 4098    3   22  118   48   27  446    9 2161 5220    7   52  160    3\n",
            "   39  960  130  807   18  174  366   24  393    7 1295   14   49   51\n",
            "   10  289    2   19    9   13  488    2  397  983   18   10  102    2\n",
            "  410   13 1172   15    2 2131   14    9    7  443   20    2  267 3775\n",
            " 3322    3   47   13  450    4   19   90   42 2193   16   12  410    2\n",
            "  164   19   18    2 2131  310    1  564   30    5    2  830   20  128\n",
            "   76  361   22   50 1336   45   22  349   93  796  478  190    9   35\n",
            "   69   75  123    1    8  998    5    2  708   20    4 2338  313    3\n",
            "   22   76  192  472    3 1302  281    4  145 1480    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "Ce texte est : positif\n",
            "<class 'numpy.ndarray'>\n",
            "[ 165  272    8    4  132   10   76  863   33    1   19   40   14    4\n",
            "   95    6 5249    3    1   54  339   16  155  659   11    7  133   10\n",
            "  196    4 1004    5 7416 2724 2843   21    4  490   19   18    9  158\n",
            "   69  472    3 1949   69    6    1   15    4  132   28    5  129  476\n",
            "   10   76  235  796 4742    5    2 2462 3611    3    4  166 6003 7333\n",
            "   91   15   72    2  164  273    8   97  401   10 1504   11   19   84\n",
            "   10  455    6   26 2098  202  108 5286   15    2   88  170   10  184\n",
            "   48   10  455    2  111   13  138  358    2  402    5    4  826  418\n",
            "    1  375    3    2 6294    1  109   13   52 1791  741   21   80   15\n",
            "    4 1671 2304 5858    3 2373  516   47   65    4  166  331  735   20\n",
            "    2  170    5    2    1  778   34 1253    6   75   46  179   49  151\n",
            "    6  293    2   80  461  460   88    5   93  823  179    1 1572   38\n",
            "   88 1691 1741   94    2 1036   22   75   81    2   17    2   50 4119\n",
            "   47    7   20  216    3    2  111   60  269  556    6  866   16    1\n",
            "  218   14  906   14]\n",
            "Ce texte est : negatif\n",
            "<class 'numpy.ndarray'>\n",
            "[  29  354 8264    1 1920 8832   31   29 1753   29    2  244   12   71\n",
            "  110   67   86  100    3   86  402 4805    7    1 1896    7   37  126\n",
            "    2    1    1  402   23    4    1    7   48   10  115   42   49 1219\n",
            "  110 1526    6 9270  244   31    1  703   65    1   13   20  213    3\n",
            "    2    1 4492    1   13    4    1 2918    1    1   14  887    1    1\n",
            "  152   25  187  402   18  577    2  322  125 1454    3  244   51   20\n",
            "  277   10   59  115    6  118   87    2  314   15   11   17  371   56\n",
            "  110   25   10  105    4  173    5   79   25   37   72  244    8  249\n",
            "  220  159    1    7    4  219   25    3   10  237 1064   15    2  287\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "Ce texte est : positif\n",
            "<class 'numpy.ndarray'>\n",
            "[  11    7    2  236    5   17   12  475    6   26   49   18 1850   83\n",
            "  150   48    2  550   23  143 4822  258    6   82   16    2  413   10\n",
            "  102    2  328  144  295    6  896    2 8464    5    2  877    1  225\n",
            "  131    6    2  247   12   34  498   24 4597   51   55  903  133   27\n",
            "  475    6  137  142   15   93   18  205  101   11   27  392    4 1113\n",
            "    6    2 7149 3288 2799    3  475   85    6  137  579    1    2 1771\n",
            "  356  114    7    2 4597   47   10  376  342  151  219 1008   64 1625\n",
            "   18  133   21   40  193   93    2  279   12  294    6   96    4  377\n",
            "   17   38   12   41  193   12  279    6    4    1  449 9938    3  577\n",
            "    5   93  682   87    6 5223    2  377 4822  225 3209    8    2    1\n",
            " 1236   38    4  932   34   30  178    6  162 1671   18   34   30 2611\n",
            "   28   50  150   10   89  256   12  212   57    1 6316    8    2  413\n",
            "    2  328   23  258  440 3413   18   34   30 2794   48  560   45  212\n",
            "    4  973    3    2    1 2452 2892    7  362    9    7 4160    6   21\n",
            "   25   33    1 6316]\n",
            "Ce texte est : negatif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Le Modèle : RNN"
      ],
      "metadata": {
        "id": "xDd25f0bNFGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "d-dVx5iQUw0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_RNN():\n",
        "  max_tokens = 10000\n",
        "  embed_dimension = 256\n",
        "  input = tf.keras.Input(shape=(None,),dtype='int64')\n",
        "  ###Extraction de caractèristiques\n",
        "  x = layers.Embedding(max_tokens,embed_dimension)(input)\n",
        "  x = layers.SimpleRNN(64,activation='tanh')(x)\n",
        "  ###Classifieur\n",
        "  predictions = layers.Dense(1,activation='sigmoid')(x)\n",
        "  model = tf.keras.Model(input,predictions)\n",
        "  return model"
      ],
      "metadata": {
        "id": "97MQUbPZvigY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_RNN()\n",
        "model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "wFQM9gX9vGmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-_pzzLQVYMM",
        "outputId": "62ea301b-f5bb-4ed7-84e0-6e476cf8c4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_5 (Embedding)     (None, None, 256)         2560000   \n",
            "                                                                 \n",
            " simple_rnn_3 (SimpleRNN)    (None, 64)                20544     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,580,609\n",
            "Trainable params: 2,580,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(vector_train_dataset,validation_data=vector_val_dataset,epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWnMn-wbbTG8",
        "outputId": "f1162b15-5671-4396-fc41-32356ab80543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "715/715 [==============================] - 186s 257ms/step - loss: 0.6915 - accuracy: 0.5218 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
            "Epoch 2/10\n",
            "715/715 [==============================] - 150s 210ms/step - loss: 0.6814 - accuracy: 0.5666 - val_loss: 0.6905 - val_accuracy: 0.5436\n",
            "Epoch 3/10\n",
            "715/715 [==============================] - 151s 211ms/step - loss: 0.6273 - accuracy: 0.6709 - val_loss: 0.6939 - val_accuracy: 0.5558\n",
            "Epoch 4/10\n",
            "715/715 [==============================] - 151s 211ms/step - loss: 0.4965 - accuracy: 0.7807 - val_loss: 0.7221 - val_accuracy: 0.5606\n",
            "Epoch 5/10\n",
            "715/715 [==============================] - 145s 203ms/step - loss: 0.3259 - accuracy: 0.8868 - val_loss: 0.7856 - val_accuracy: 0.5664\n",
            "Epoch 6/10\n",
            "715/715 [==============================] - 144s 201ms/step - loss: 0.1936 - accuracy: 0.9444 - val_loss: 0.8614 - val_accuracy: 0.5774\n",
            "Epoch 7/10\n",
            "715/715 [==============================] - 145s 202ms/step - loss: 0.1077 - accuracy: 0.9774 - val_loss: 0.9650 - val_accuracy: 0.5652\n",
            "Epoch 8/10\n",
            "715/715 [==============================] - 154s 216ms/step - loss: 0.0614 - accuracy: 0.9902 - val_loss: 1.0633 - val_accuracy: 0.5560\n",
            "Epoch 9/10\n",
            "715/715 [==============================] - 145s 203ms/step - loss: 0.0386 - accuracy: 0.9948 - val_loss: 1.1421 - val_accuracy: 0.5622\n",
            "Epoch 10/10\n",
            "715/715 [==============================] - 145s 202ms/step - loss: 0.0270 - accuracy: 0.9967 - val_loss: 1.2300 - val_accuracy: 0.5550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_test_dataset = test_dataset.map(vectoriser_texte)\n",
        "model.evaluate(vector_test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exw39uYib8hX",
        "outputId": "4c472cbe-d3ed-4219-ca0c-ffbcf3029d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "893/893 [==============================] - 8s 9ms/step - loss: 0.9135 - accuracy: 0.7274\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9135347008705139, 0.7273600101470947]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bidirectionnal RNN"
      ],
      "metadata": {
        "id": "qetXybQbWb27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_biRNN():\n",
        "  max_tokens = 10000\n",
        "  embed_dimension = 256\n",
        "  input = tf.keras.Input(shape=(None,),dtype='int64')\n",
        "  ###Extraction de caractèristiques\n",
        "  x = layers.Embedding(max_tokens,embed_dimension)(input)\n",
        "  x = layers.Bidirectional(layers.SimpleRNN(64,activation='tanh'))(x)\n",
        "  ###Classifieur\n",
        "  predictions = layers.Dense(1,activation='sigmoid')(x)\n",
        "  model = tf.keras.Model(input,predictions)\n",
        "  return model\n",
        "\n",
        "model = model_biRNN()\n",
        "model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "FdWpj5BQkJEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a765ebd1-9ced-4150-fd6c-c01cac6d47fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_6 (Embedding)     (None, None, 256)         2560000   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 128)              41088     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,601,217\n",
            "Trainable params: 2,601,217\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(vector_train_dataset,validation_data=vector_val_dataset,epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXi1Xiu1XmEV",
        "outputId": "a44ce0a3-3faa-4ea9-b313-85689e441dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "715/715 [==============================] - 298s 413ms/step - loss: 0.6779 - accuracy: 0.5758 - val_loss: 0.6729 - val_accuracy: 0.5880\n",
            "Epoch 2/10\n",
            "715/715 [==============================] - 265s 371ms/step - loss: 0.6354 - accuracy: 0.6423 - val_loss: 0.6302 - val_accuracy: 0.6450\n",
            "Epoch 3/10\n",
            "715/715 [==============================] - 254s 355ms/step - loss: 0.4669 - accuracy: 0.7979 - val_loss: 0.6131 - val_accuracy: 0.6682\n",
            "Epoch 4/10\n",
            "715/715 [==============================] - 254s 355ms/step - loss: 0.2832 - accuracy: 0.9049 - val_loss: 0.4044 - val_accuracy: 0.8240\n",
            "Epoch 5/10\n",
            "715/715 [==============================] - 251s 350ms/step - loss: 0.1509 - accuracy: 0.9531 - val_loss: 0.3906 - val_accuracy: 0.8278\n",
            "Epoch 6/10\n",
            "715/715 [==============================] - 251s 351ms/step - loss: 0.0784 - accuracy: 0.9813 - val_loss: 0.4055 - val_accuracy: 0.8256\n",
            "Epoch 7/10\n",
            "715/715 [==============================] - 250s 350ms/step - loss: 0.0370 - accuracy: 0.9933 - val_loss: 0.4196 - val_accuracy: 0.8360\n",
            "Epoch 8/10\n",
            "715/715 [==============================] - 252s 352ms/step - loss: 0.0170 - accuracy: 0.9977 - val_loss: 0.4557 - val_accuracy: 0.8324\n",
            "Epoch 9/10\n",
            "715/715 [==============================] - 253s 354ms/step - loss: 0.0101 - accuracy: 0.9990 - val_loss: 0.4770 - val_accuracy: 0.8314\n",
            "Epoch 10/10\n",
            "715/715 [==============================] - 252s 353ms/step - loss: 0.0057 - accuracy: 0.9996 - val_loss: 0.5080 - val_accuracy: 0.8366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM\n",
        "\n",
        "#### LSTM networks are particularly effective in processing and predicting sequences of data, such as speech, text, and time series data. They are designed to address the vanishing gradient problem, which occurs when the gradients in the backpropagation algorithm diminish exponentially over time, making it difficult for traditional RNNs to capture long-term dependencies.\n",
        "\n",
        "#### The key idea behind LSTM is the introduction of memory cells, which are responsible for storing and retrieving information over extended time intervals. The memory cells are composed of three main components: an input gate, a forget gate, and an output gate. \n",
        "\n",
        "#### Input Gate: It determines how much new information should be stored in the memory cells. It takes input from the current time step and the previous hidden state and applies a sigmoid activation function to produce values between 0 and 1. A value close to 0 means that the information should be ignored, while a value close to 1 means that the information should be stored.\n",
        "\n",
        "#### Forget Gate: It decides what information should be discarded from the memory cells. It takes input from the current time step and the previous hidden state and applies a sigmoid activation function. This gate outputs values between 0 and 1, with 0 indicating that the information should be forgotten and 1 indicating that the information should be retained.\n",
        "\n",
        "#### Output Gate: It determines how much information from the memory cells should be exposed as the output of the current time step. It takes input from the current time step and the previous hidden state, and applies a sigmoid activation function. Additionally, it also applies the hyperbolic tangent (tanh) activation function to the current memory cell values. The output gate produces values between 0 and 1, indicating the amount of information to expose."
      ],
      "metadata": {
        "id": "5IexdY7uHu31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Equation of LSTM\n",
        "\n",
        "###### i(t) = σ(Wi * h(t-1)+ Ui* x(t) + bi)  input gate\n",
        "###### f(t) = σ(Wf * h(t-1)+ Ui* x(t) + bf)  forget gate\n",
        "###### g(t) = tanh(Wg * h(t-1)+ Ui* x(t) + b)  candidate memory cell\n",
        "###### c(t) = f(t) * c(t-1) + i(t) * g(t)  memory cell\n",
        "###### o(t) = σ(Wo * h(t-1)+ Ui* x(t) + bo)  output gate\n",
        "###### h(t) = o(t) * tanh(c(t))  shadow state\n",
        "###### y(t) = h(t)  cell output"
      ],
      "metadata": {
        "id": "bJCM8aaJmNJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_LSTM():\n",
        "  max_tokens = 10000\n",
        "  embed_dimension = 256\n",
        "  input = tf.keras.Input(shape=(None,),dtype='int64')\n",
        "  ###Extraction de caractèristiques\n",
        "  x = layers.Embedding(max_tokens,embed_dimension)(input)\n",
        "  x = layers.Dropout(0.2)(x)\n",
        "  x = layers.LSTM(64)(x)\n",
        "  ###Classifieur\n",
        "  predictions = layers.Dense(1,activation='sigmoid')(x)\n",
        "  model = tf.keras.Model(input,predictions)\n",
        "  return model"
      ],
      "metadata": {
        "id": "-ItcWhyFHv9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_LSTM()\n",
        "model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "MP9VUNg9L8A2",
        "outputId": "38402303-6f21-4907-ba63-a54aaa66aa25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0b4f75c1835d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-6e1d36756d0a>\u001b[0m in \u001b[0;36mmodel_LSTM\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m###Extraction de caractèristiques\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membed_dimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(vector_train_dataset,validation_data=vector_val_dataset,epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io2ch5aLMAUj",
        "outputId": "5a641fe5-880f-4f68-9a51-bbd08147244c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "715/715 [==============================] - 54s 70ms/step - loss: 0.6925 - accuracy: 0.5153 - val_loss: 0.6895 - val_accuracy: 0.5316\n",
            "Epoch 2/10\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.4942 - accuracy: 0.7797 - val_loss: 0.4082 - val_accuracy: 0.8364\n",
            "Epoch 3/10\n",
            "715/715 [==============================] - 14s 20ms/step - loss: 0.3091 - accuracy: 0.8834 - val_loss: 0.3267 - val_accuracy: 0.8648\n",
            "Epoch 4/10\n",
            "715/715 [==============================] - 12s 17ms/step - loss: 0.2314 - accuracy: 0.9174 - val_loss: 0.3501 - val_accuracy: 0.8576\n",
            "Epoch 5/10\n",
            "715/715 [==============================] - 13s 18ms/step - loss: 0.1876 - accuracy: 0.9364 - val_loss: 0.3530 - val_accuracy: 0.8658\n",
            "Epoch 6/10\n",
            "715/715 [==============================] - 12s 17ms/step - loss: 0.1627 - accuracy: 0.9474 - val_loss: 0.3503 - val_accuracy: 0.8608\n",
            "Epoch 7/10\n",
            "715/715 [==============================] - 11s 16ms/step - loss: 0.1320 - accuracy: 0.9614 - val_loss: 0.4833 - val_accuracy: 0.8552\n",
            "Epoch 8/10\n",
            "715/715 [==============================] - 11s 15ms/step - loss: 0.1261 - accuracy: 0.9627 - val_loss: 0.4563 - val_accuracy: 0.8552\n",
            "Epoch 9/10\n",
            "715/715 [==============================] - 11s 15ms/step - loss: 0.1092 - accuracy: 0.9699 - val_loss: 0.5385 - val_accuracy: 0.8484\n",
            "Epoch 10/10\n",
            "715/715 [==============================] - 10s 14ms/step - loss: 0.1039 - accuracy: 0.9720 - val_loss: 0.5380 - val_accuracy: 0.8444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bidirectional LSTM"
      ],
      "metadata": {
        "id": "cgv6uPGoVD7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_biLSTM():\n",
        "  max_tokens = 10000\n",
        "  embed_dimension = 256\n",
        "  input = tf.keras.Input(shape=(None,),dtype='int64')\n",
        "  ###Extraction de caractèristiques\n",
        "  x = layers.Embedding(max_tokens,embed_dimension)(input)\n",
        "  x = layers.Dropout(0.2)(x)\n",
        "  x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "  ###Classifieur\n",
        "  predictions = layers.Dense(1,activation='sigmoid')(x)\n",
        "  model = tf.keras.Model(input,predictions)\n",
        "  return model\n",
        "\n",
        "model = model_biLSTM()\n",
        "model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnBOCiknRCpR",
        "outputId": "67a2db38-40f1-40f7-dbe3-e449f6384c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 256)         2560000   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 256)         0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 128)              164352    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,724,481\n",
            "Trainable params: 2,724,481\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(vector_train_dataset,validation_data=vector_val_dataset,epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAjTSBuankno",
        "outputId": "e1237457-4876-4dca-b4e5-1d2a92a0fd8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "715/715 [==============================] - 82s 102ms/step - loss: 0.5611 - accuracy: 0.6959 - val_loss: 0.3715 - val_accuracy: 0.8464\n",
            "Epoch 2/10\n",
            "715/715 [==============================] - 20s 28ms/step - loss: 0.3104 - accuracy: 0.8812 - val_loss: 0.3221 - val_accuracy: 0.8668\n",
            "Epoch 3/10\n",
            "715/715 [==============================] - 22s 31ms/step - loss: 0.2331 - accuracy: 0.9139 - val_loss: 0.3148 - val_accuracy: 0.8654\n",
            "Epoch 4/10\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.1893 - accuracy: 0.9341 - val_loss: 0.3544 - val_accuracy: 0.8604\n",
            "Epoch 5/10\n",
            "715/715 [==============================] - 19s 27ms/step - loss: 0.1560 - accuracy: 0.9478 - val_loss: 0.4083 - val_accuracy: 0.8616\n",
            "Epoch 6/10\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.1333 - accuracy: 0.9568 - val_loss: 0.3933 - val_accuracy: 0.8534\n",
            "Epoch 7/10\n",
            "715/715 [==============================] - 17s 23ms/step - loss: 0.1150 - accuracy: 0.9636 - val_loss: 0.4696 - val_accuracy: 0.8558\n",
            "Epoch 8/10\n",
            "715/715 [==============================] - 16s 22ms/step - loss: 0.0968 - accuracy: 0.9704 - val_loss: 0.4538 - val_accuracy: 0.8542\n",
            "Epoch 9/10\n",
            "715/715 [==============================] - 16s 23ms/step - loss: 0.0880 - accuracy: 0.9747 - val_loss: 0.4811 - val_accuracy: 0.8474\n",
            "Epoch 10/10\n",
            "715/715 [==============================] - 15s 21ms/step - loss: 0.0760 - accuracy: 0.9771 - val_loss: 0.4839 - val_accuracy: 0.8536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OtZ14WwQc43k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU\n",
        "\n",
        "##### GRU was introduced as a way to address some of the complexities and computational overhead associated with LSTM while still capturing long-term dependencies in sequential data.\n",
        "\n",
        "##### Update Gate (z): It determines how much of the previous hidden state should be passed along to the current time step. It takes input from the current time step and the previous hidden state and applies a sigmoid activation function. A value close to 1 means that the previous hidden state should be fully retained, while a value close to 0 means that the previous hidden state should be mostly ignored.\n",
        "\n",
        "##### Reset Gate (r): It controls how much of the previous hidden state is used to compute the current hidden state candidate. It takes input from the current time step and the previous hidden state and applies a sigmoid activation function. A value close to 1 means that the previous hidden state should be fully used, while a value close to 0 means that the previous hidden state should be mostly ignored."
      ],
      "metadata": {
        "id": "6g2MMpuWqeO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_biGRU():\n",
        "  max_tokens = 10000\n",
        "  embed_dimension = 256\n",
        "  input = tf.keras.Input(shape=(None,),dtype='int64')\n",
        "  ###Extraction de caractèristiques\n",
        "  x = layers.Embedding(max_tokens,embed_dimension)(input)\n",
        "  x = layers.Bidirectional(layers.GRU(64))(x)\n",
        "  ###Classifieur\n",
        "  predictions = layers.Dense(1,activation='sigmoid')(x)\n",
        "  model = tf.keras.Model(input,predictions)\n",
        "  return model\n",
        "\n",
        "model = model_biGRU()\n",
        "model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l329-CVJoIf5",
        "outputId": "ee219ec6-2ea5-4e84-d98a-2d4c4b08447a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_9 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_8 (Embedding)     (None, None, 256)         2560000   \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 128)              123648    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,683,777\n",
            "Trainable params: 2,683,777\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(vector_train_dataset,validation_data=vector_val_dataset,epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgX3xf7nqtND",
        "outputId": "8c03432d-3574-4da7-ec9d-4bb0d131d087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "715/715 [==============================] - 59s 77ms/step - loss: 0.5956 - accuracy: 0.6526 - val_loss: 0.4016 - val_accuracy: 0.8266\n",
            "Epoch 2/10\n",
            "715/715 [==============================] - 21s 29ms/step - loss: 0.3087 - accuracy: 0.8772 - val_loss: 0.3340 - val_accuracy: 0.8602\n",
            "Epoch 3/10\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.2262 - accuracy: 0.9175 - val_loss: 0.3368 - val_accuracy: 0.8620\n",
            "Epoch 4/10\n",
            "715/715 [==============================] - 16s 22ms/step - loss: 0.1812 - accuracy: 0.9380 - val_loss: 0.3644 - val_accuracy: 0.8560\n",
            "Epoch 5/10\n",
            "715/715 [==============================] - 14s 20ms/step - loss: 0.1479 - accuracy: 0.9537 - val_loss: 0.3747 - val_accuracy: 0.8490\n",
            "Epoch 6/10\n",
            "715/715 [==============================] - 17s 23ms/step - loss: 0.1240 - accuracy: 0.9618 - val_loss: 0.4166 - val_accuracy: 0.8550\n",
            "Epoch 7/10\n",
            "715/715 [==============================] - 14s 20ms/step - loss: 0.1046 - accuracy: 0.9692 - val_loss: 0.4391 - val_accuracy: 0.8528\n",
            "Epoch 8/10\n",
            "715/715 [==============================] - 15s 21ms/step - loss: 0.0883 - accuracy: 0.9750 - val_loss: 0.5592 - val_accuracy: 0.8434\n",
            "Epoch 9/10\n",
            "715/715 [==============================] - 14s 19ms/step - loss: 0.0812 - accuracy: 0.9769 - val_loss: 0.5690 - val_accuracy: 0.8474\n",
            "Epoch 10/10\n",
            "715/715 [==============================] - 15s 20ms/step - loss: 0.0653 - accuracy: 0.9819 - val_loss: 0.5739 - val_accuracy: 0.8462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ctj1hcn-qu_D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}